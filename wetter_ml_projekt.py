# -*- coding: utf-8 -*-
"""Wetter ML Projekt.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pVD_ljrf36eC33rlev3EsCnL1KA_-9dN

# resources

Find data files here: https://www.ncei.noaa.gov/cdo-web/datatools/lcd

https://cds.climate.copernicus.eu/cdsapp#!/search?type=dataset

https://www.youtube.com/watch?v=6QjSQBvNLdQ&list=WL&index=3

https://www.ncei.noaa.gov/pub/data/cdo/documentation/LCD_documentation.pdf

# Objective & Plan

Project Plan: West Lafayette, Indiana Weather Prediction Using Machine Learning with Consideration of Climate Change

A coding project which uses machine learning and data from decades of weather patterns of each day to determine what the properties of the weather will be for each day while also taking into account climate change.

#### Goals
1. **Primary Goal**: Develop a machine learning model to predict daily weather patterns using historical weather data while accounting for climate change effects.
2. **Secondary Goal**: Enhance the model's accuracy and robustness with advanced techniques and external climate data.

#### Tasks and Completion Plan

1. **Data Collection**
   - **Task**: Gather historical weather data spanning several decades.
   - **How to Complete**:
     - Use sources like NOAA, NASA, and other meteorological databases.
     - Download data in CSV format.
   - **Potential Issues**:
     - Inconsistent data formats.
     - Missing data points.

2. **Data Preprocessing**
   - **Task**: Clean and preprocess the data for analysis.
   - **How to Complete**:
     - Handle missing values using imputation techniques.
     - Normalize/standardize data for uniformity.
     - Convert categorical data to numerical if necessary.
   - **Potential Issues**:
     - Handling large datasets may require substantial memory and processing power.
     - Identifying and dealing with outliers.

3. **Feature Engineering**
   - **Task**: Extract and create relevant features from the dataset.
   - **How to Complete**:
     - Identify key weather parameters (e.g., temperature, humidity, wind speed).
     - Create new features like moving averages, seasonal indicators.
     - Incorporate external climate change data (e.g., CO2 levels, temperature anomalies).
   - **Potential Issues**:
     - Determining the most relevant features.
     - Overfitting with too many features.

4. **Model Selection**
   - **Task**: Choose and implement suitable machine learning models.
   - **How to Complete**:
     - Start with simple models (e.g., linear regression, decision trees).
     - Progress to more complex models (e.g., random forests, LSTM networks).
     - Use Scikit-learn and TensorFlow/PyTorch for model implementation.
   - **Potential Issues**:
     - Model complexity leading to overfitting or underfitting.
     - Computational resources for training complex models.

5. **Model Training**
   - **Task**: Train the selected models on the preprocessed data.
   - **How to Complete**:
     - Split data into training and testing sets.
     - Use cross-validation for robust performance estimation.
     - Tune hyperparameters using grid search or randomized search.
   - **Potential Issues**:
     - Long training times for large datasets.
     - Overfitting or underfitting during training.

6. **Model Evaluation**
   - **Task**: Evaluate the performance of the trained models.
   - **How to Complete**:
     - Use metrics such as mean squared error (MSE), mean absolute error (MAE), and R-squared.
     - Compare model predictions with actual data.
   - **Potential Issues**:
     - Ensuring that evaluation metrics reflect the model's real-world performance.
     - Balancing between bias and variance.

7. **Incorporate Climate Change Factors**
   - **Task**: Integrate climate change data into the model to account for long-term trends.
   - **How to Complete**:
     - Obtain climate change data (e.g., CO2 levels, temperature anomalies).
     - Include these factors as features in the model.
   - **Potential Issues**:
     - Finding reliable and comprehensive climate change datasets.
     - Integrating these datasets with weather data effectively.

8. **Final Model Selection and Fine-Tuning**
   - **Task**: Select the best-performing model and fine-tune it.
   - **How to Complete**:
     - Analyze evaluation results to choose the best model.
     - Perform additional fine-tuning to optimize performance.
   - **Potential Issues**:
     - Overfitting during the fine-tuning process.
     - Ensuring model generalization to unseen data.

9. **Deployment**
   - **Task**: Deploy the final model for making real-time predictions.
   - **How to Complete**:
     - Create a web application or API to serve the model.
     - Use Flask/Django for web deployment, or FastAPI for creating APIs.
   - **Potential Issues**:
     - Ensuring scalability and performance of the deployed model.
     - Handling real-time data efficiently.

#### Potential Issues and Mitigations

- **Data Quality**: Ensure thorough data cleaning and preprocessing.
- **Model Overfitting/Underfitting**: Use cross-validation and regularization techniques.
- **Computational Resources**: Utilize cloud services (e.g., AWS, Google Cloud) for training and deployment.
- **Integration of Climate Data**: Careful feature engineering and validation.

#### Extra Features to Add

1. **Interactive Visualizations**
   - Create interactive dashboards to visualize weather predictions and climate trends using Plotly or Dash.

2. **Model Explanation**
   - Implement techniques like SHAP (SHapley Additive exPlanations) to explain model predictions and understand feature importance.

3. **Automated Data Pipeline**
   - Develop an automated data pipeline to continuously update the model with new data.

4. **Ensemble Models**
   - Combine predictions from multiple models to improve accuracy and robustness.

5. **Weather Alerts**
   - Integrate a notification system to alert users about significant weather changes or extreme weather events.

6. **User Interface**
   - Develop a user-friendly interface for non-technical users to interact with the model and view predictions.

This plan outlines the steps you need to take to complete your project, the goals, potential issues, and additional features that could enhance your project. Good luck with your coding project!
"""

#important pandas stuff to know/use

df.decribe()
df.info()

#look into data mine code as well!

"""# The Code"""

import pandas as pd
pd.set_option('display.max_columns', None)
df = pd.read_csv("/content/LCD_USW00003889_2006.csv")
print(df.head(8))  # Display the first few rows of the DataFrame

# Access the column names of the DataFrame
column_names = df.columns

for name in column_names:
  print(name)
#print("Names of every column in the DataFrame:")
#print(column_names)









# Step 1: Import libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# Step 2: Load and preprocess data
data = pd.read_csv('your_data.csv')
X = data.drop(columns=['target_column'])
y = data['target_column']

# Step 3: Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Define preprocessing steps
preprocessor = Pipeline([
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

# Step 5: Define the model
model = RandomForestRegressor(n_estimators=100, random_state=42)

# Step 6: Create a pipeline
pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('model', model)
])

# Step 7: Train the model
pipeline.fit(X_train, y_train)

# Step 8: Evaluate the model
y_pred = pipeline.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print('Mean Squared Error:', mse)

# Step 9: Deploy the model (optional)
# Deploy your model for making predictions on new data

"""# testing"""

import pandas as pd
pd.set_option('display.max_columns', None)
df = pd.read_csv("/content/2024_test_3738819.csv")
print(df.head(8))  # Display the first few rows of the DataFrame

column_names = df.columns

for name in column_names:
    print(name)